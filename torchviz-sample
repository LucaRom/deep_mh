digraph {
	graph [size="72.45,72.45"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	3115521373264 [label="
 (3, 3, 161, 161)" fillcolor=darkolivegreen1]
	3115521548976 [label=ConvolutionBackward0]
	3115521549072 -> 3115521548976
	3115521549072 [label=ReluBackward0]
	3115521549120 -> 3115521549072
	3115521549120 [label=NativeBatchNormBackward0]
	3115521549216 -> 3115521549120
	3115521549216 [label=ConvolutionBackward0]
	3115521549408 -> 3115521549216
	3115521549408 [label=ReluBackward0]
	3115521549600 -> 3115521549408
	3115521549600 [label=NativeBatchNormBackward0]
	3115521549696 -> 3115521549600
	3115521549696 [label=ConvolutionBackward0]
	3115521549888 -> 3115521549696
	3115521549888 [label=CatBackward0]
	3115521550080 -> 3115521549888
	3115521550080 [label=ReluBackward0]
	3115521550224 -> 3115521550080
	3115521550224 [label=NativeBatchNormBackward0]
	3115521550320 -> 3115521550224
	3115521550320 [label=ConvolutionBackward0]
	3115521550512 -> 3115521550320
	3115521550512 [label=ReluBackward0]
	3115521550704 -> 3115521550512
	3115521550704 [label=NativeBatchNormBackward0]
	3115521550800 -> 3115521550704
	3115521550800 [label=ConvolutionBackward0]
	3115521550992 -> 3115521550800
	3115446461760 [label="layers.0.net.0.weight
 (64, 3, 3, 3)" fillcolor=lightblue]
	3115446461760 -> 3115521550992
	3115521550992 [label=AccumulateGrad]
	3115521550944 -> 3115521550800
	3115446461840 [label="layers.0.net.0.bias
 (64)" fillcolor=lightblue]
	3115446461840 -> 3115521550944
	3115521550944 [label=AccumulateGrad]
	3115521550752 -> 3115521550704
	3115446461920 [label="layers.0.net.1.weight
 (64)" fillcolor=lightblue]
	3115446461920 -> 3115521550752
	3115521550752 [label=AccumulateGrad]
	3115521550608 -> 3115521550704
	3115446462000 [label="layers.0.net.1.bias
 (64)" fillcolor=lightblue]
	3115446462000 -> 3115521550608
	3115521550608 [label=AccumulateGrad]
	3115521550464 -> 3115521550320
	3115446462320 [label="layers.0.net.3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	3115446462320 -> 3115521550464
	3115521550464 [label=AccumulateGrad]
	3115521550416 -> 3115521550320
	3115446462400 [label="layers.0.net.3.bias
 (64)" fillcolor=lightblue]
	3115446462400 -> 3115521550416
	3115521550416 [label=AccumulateGrad]
	3115521550272 -> 3115521550224
	3115446462480 [label="layers.0.net.4.weight
 (64)" fillcolor=lightblue]
	3115446462480 -> 3115521550272
	3115521550272 [label=AccumulateGrad]
	3115521550128 -> 3115521550224
	3115446462560 [label="layers.0.net.4.bias
 (64)" fillcolor=lightblue]
	3115446462560 -> 3115521550128
	3115521550128 [label=AccumulateGrad]
	3115521550032 -> 3115521549888
	3115521550032 [label=ConstantPadNdBackward0]
	3115521550560 -> 3115521550032
	3115521550560 [label=ConvolutionBackward0]
	3115521550848 -> 3115521550560
	3115521550848 [label=UpsampleBilinear2DBackward1]
	3115521551136 -> 3115521550848
	3115521551136 [label=ReluBackward0]
	3115521551232 -> 3115521551136
	3115521551232 [label=NativeBatchNormBackward0]
	3115521551328 -> 3115521551232
	3115521551328 [label=ConvolutionBackward0]
	3115521551520 -> 3115521551328
	3115521551520 [label=ReluBackward0]
	3115521551712 -> 3115521551520
	3115521551712 [label=NativeBatchNormBackward0]
	3115521551808 -> 3115521551712
	3115521551808 [label=ConvolutionBackward0]
	3115521552000 -> 3115521551808
	3115521552000 [label=CatBackward0]
	3115521552192 -> 3115521552000
	3115521552192 [label=ReluBackward0]
	3115521552336 -> 3115521552192
	3115521552336 [label=NativeBatchNormBackward0]
	3115521552240 -> 3115521552336
	3115521552240 [label=ConvolutionBackward0]
	3115521581360 -> 3115521552240
	3115521581360 [label=ReluBackward0]
	3115521581552 -> 3115521581360
	3115521581552 [label=NativeBatchNormBackward0]
	3115521581648 -> 3115521581552
	3115521581648 [label=ConvolutionBackward0]
	3115521581840 -> 3115521581648
	3115521581840 [label=MaxPool2DWithIndicesBackward0]
	3115521550080 -> 3115521581840
	3115521581792 -> 3115521581648
	3115446462960 [label="layers.1.net.1.net.0.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	3115446462960 -> 3115521581792
	3115521581792 [label=AccumulateGrad]
	3115521581744 -> 3115521581648
	3115446463040 [label="layers.1.net.1.net.0.bias
 (128)" fillcolor=lightblue]
	3115446463040 -> 3115521581744
	3115521581744 [label=AccumulateGrad]
	3115521581600 -> 3115521581552
	3115446463120 [label="layers.1.net.1.net.1.weight
 (128)" fillcolor=lightblue]
	3115446463120 -> 3115521581600
	3115521581600 [label=AccumulateGrad]
	3115521581456 -> 3115521581552
	3115446463200 [label="layers.1.net.1.net.1.bias
 (128)" fillcolor=lightblue]
	3115446463200 -> 3115521581456
	3115521581456 [label=AccumulateGrad]
	3115521581312 -> 3115521552240
	3115446463600 [label="layers.1.net.1.net.3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	3115446463600 -> 3115521581312
	3115521581312 [label=AccumulateGrad]
	3115521581264 -> 3115521552240
	3115446463680 [label="layers.1.net.1.net.3.bias
 (128)" fillcolor=lightblue]
	3115446463680 -> 3115521581264
	3115521581264 [label=AccumulateGrad]
	3115521581168 -> 3115521552336
	3115446463760 [label="layers.1.net.1.net.4.weight
 (128)" fillcolor=lightblue]
	3115446463760 -> 3115521581168
	3115521581168 [label=AccumulateGrad]
	3115521581120 -> 3115521552336
	3115446463840 [label="layers.1.net.1.net.4.bias
 (128)" fillcolor=lightblue]
	3115446463840 -> 3115521581120
	3115521581120 [label=AccumulateGrad]
	3115521552144 -> 3115521552000
	3115521552144 [label=ConstantPadNdBackward0]
	3115521552288 -> 3115521552144
	3115521552288 [label=ConvolutionBackward0]
	3115521581696 -> 3115521552288
	3115521581696 [label=UpsampleBilinear2DBackward1]
	3115521581936 -> 3115521581696
	3115521581936 [label=ReluBackward0]
	3115521582128 -> 3115521581936
	3115521582128 [label=NativeBatchNormBackward0]
	3115521582224 -> 3115521582128
	3115521582224 [label=ConvolutionBackward0]
	3115521582416 -> 3115521582224
	3115521582416 [label=ReluBackward0]
	3115521582608 -> 3115521582416
	3115521582608 [label=NativeBatchNormBackward0]
	3115521582704 -> 3115521582608
	3115521582704 [label=ConvolutionBackward0]
	3115521582896 -> 3115521582704
	3115521582896 [label=CatBackward0]
	3115521583088 -> 3115521582896
	3115521583088 [label=ReluBackward0]
	3115521583232 -> 3115521583088
	3115521583232 [label=NativeBatchNormBackward0]
	3115521583328 -> 3115521583232
	3115521583328 [label=ConvolutionBackward0]
	3115521583520 -> 3115521583328
	3115521583520 [label=ReluBackward0]
	3115521583712 -> 3115521583520
	3115521583712 [label=NativeBatchNormBackward0]
	3115521583808 -> 3115521583712
	3115521583808 [label=ConvolutionBackward0]
	3115521584000 -> 3115521583808
	3115521584000 [label=MaxPool2DWithIndicesBackward0]
	3115521552192 -> 3115521584000
	3115521583952 -> 3115521583808
	3115446464240 [label="layers.2.net.1.net.0.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	3115446464240 -> 3115521583952
	3115521583952 [label=AccumulateGrad]
	3115521583904 -> 3115521583808
	3115446464320 [label="layers.2.net.1.net.0.bias
 (256)" fillcolor=lightblue]
	3115446464320 -> 3115521583904
	3115521583904 [label=AccumulateGrad]
	3115521583760 -> 3115521583712
	3115446464400 [label="layers.2.net.1.net.1.weight
 (256)" fillcolor=lightblue]
	3115446464400 -> 3115521583760
	3115521583760 [label=AccumulateGrad]
	3115521583616 -> 3115521583712
	3115446550592 [label="layers.2.net.1.net.1.bias
 (256)" fillcolor=lightblue]
	3115446550592 -> 3115521583616
	3115521583616 [label=AccumulateGrad]
	3115521583472 -> 3115521583328
	3115446550992 [label="layers.2.net.1.net.3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	3115446550992 -> 3115521583472
	3115521583472 [label=AccumulateGrad]
	3115521583424 -> 3115521583328
	3115446551072 [label="layers.2.net.1.net.3.bias
 (256)" fillcolor=lightblue]
	3115446551072 -> 3115521583424
	3115521583424 [label=AccumulateGrad]
	3115521583280 -> 3115521583232
	3115446551152 [label="layers.2.net.1.net.4.weight
 (256)" fillcolor=lightblue]
	3115446551152 -> 3115521583280
	3115521583280 [label=AccumulateGrad]
	3115521583136 -> 3115521583232
	3115446551232 [label="layers.2.net.1.net.4.bias
 (256)" fillcolor=lightblue]
	3115446551232 -> 3115521583136
	3115521583136 [label=AccumulateGrad]
	3115521583040 -> 3115521582896
	3115521583040 [label=ConstantPadNdBackward0]
	3115521583568 -> 3115521583040
	3115521583568 [label=ConvolutionBackward0]
	3115521583856 -> 3115521583568
	3115521583856 [label=UpsampleBilinear2DBackward1]
	3115521584240 -> 3115521583856
	3115521584240 [label=ReluBackward0]
	3115521584336 -> 3115521584240
	3115521584336 [label=NativeBatchNormBackward0]
	3115521584432 -> 3115521584336
	3115521584432 [label=ConvolutionBackward0]
	3115521584624 -> 3115521584432
	3115521584624 [label=ReluBackward0]
	3115521584816 -> 3115521584624
	3115521584816 [label=NativeBatchNormBackward0]
	3115521584912 -> 3115521584816
	3115521584912 [label=ConvolutionBackward0]
	3115521585104 -> 3115521584912
	3115521585104 [label=CatBackward0]
	3115521593552 -> 3115521585104
	3115521593552 [label=ReluBackward0]
	3115521593696 -> 3115521593552
	3115521593696 [label=NativeBatchNormBackward0]
	3115521593792 -> 3115521593696
	3115521593792 [label=ConvolutionBackward0]
	3115521593984 -> 3115521593792
	3115521593984 [label=ReluBackward0]
	3115521594176 -> 3115521593984
	3115521594176 [label=NativeBatchNormBackward0]
	3115521594224 -> 3115521594176
	3115521594224 [label=ConvolutionBackward0]
	3115521594512 -> 3115521594224
	3115521594512 [label=MaxPool2DWithIndicesBackward0]
	3115521583088 -> 3115521594512
	3115521594464 -> 3115521594224
	3115446551632 [label="layers.3.net.1.net.0.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	3115446551632 -> 3115521594464
	3115521594464 [label=AccumulateGrad]
	3115521594416 -> 3115521594224
	3115446551712 [label="layers.3.net.1.net.0.bias
 (512)" fillcolor=lightblue]
	3115446551712 -> 3115521594416
	3115521594416 [label=AccumulateGrad]
	3115521594080 -> 3115521594176
	3115446551792 [label="layers.3.net.1.net.1.weight
 (512)" fillcolor=lightblue]
	3115446551792 -> 3115521594080
	3115521594080 [label=AccumulateGrad]
	3115521594320 -> 3115521594176
	3115446551872 [label="layers.3.net.1.net.1.bias
 (512)" fillcolor=lightblue]
	3115446551872 -> 3115521594320
	3115521594320 [label=AccumulateGrad]
	3115521593936 -> 3115521593792
	3115446552272 [label="layers.3.net.1.net.3.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	3115446552272 -> 3115521593936
	3115521593936 [label=AccumulateGrad]
	3115521593888 -> 3115521593792
	3115446552352 [label="layers.3.net.1.net.3.bias
 (512)" fillcolor=lightblue]
	3115446552352 -> 3115521593888
	3115521593888 [label=AccumulateGrad]
	3115521593744 -> 3115521593696
	3115446552432 [label="layers.3.net.1.net.4.weight
 (512)" fillcolor=lightblue]
	3115446552432 -> 3115521593744
	3115521593744 [label=AccumulateGrad]
	3115521593600 -> 3115521593696
	3115446552512 [label="layers.3.net.1.net.4.bias
 (512)" fillcolor=lightblue]
	3115446552512 -> 3115521593600
	3115521593600 [label=AccumulateGrad]
	3115521593504 -> 3115521585104
	3115521593504 [label=ConstantPadNdBackward0]
	3115521594032 -> 3115521593504
	3115521594032 [label=ConvolutionBackward0]
	3115521594368 -> 3115521594032
	3115521594368 [label=UpsampleBilinear2DBackward1]
	3115521594848 -> 3115521594368
	3115521594848 [label=ReluBackward0]
	3115521594944 -> 3115521594848
	3115521594944 [label=NativeBatchNormBackward0]
	3115521595040 -> 3115521594944
	3115521595040 [label=ConvolutionBackward0]
	3115521595232 -> 3115521595040
	3115521595232 [label=ReluBackward0]
	3115521595424 -> 3115521595232
	3115521595424 [label=NativeBatchNormBackward0]
	3115521595520 -> 3115521595424
	3115521595520 [label=ConvolutionBackward0]
	3115521595712 -> 3115521595520
	3115521595712 [label=MaxPool2DWithIndicesBackward0]
	3115521593552 -> 3115521595712
	3115521595664 -> 3115521595520
	3115446552912 [label="layers.4.net.1.net.0.weight
 (1024, 512, 3, 3)" fillcolor=lightblue]
	3115446552912 -> 3115521595664
	3115521595664 [label=AccumulateGrad]
	3115521595616 -> 3115521595520
	3115446552992 [label="layers.4.net.1.net.0.bias
 (1024)" fillcolor=lightblue]
	3115446552992 -> 3115521595616
	3115521595616 [label=AccumulateGrad]
	3115521595472 -> 3115521595424
	3115446553072 [label="layers.4.net.1.net.1.weight
 (1024)" fillcolor=lightblue]
	3115446553072 -> 3115521595472
	3115521595472 [label=AccumulateGrad]
	3115521595328 -> 3115521595424
	3115446553152 [label="layers.4.net.1.net.1.bias
 (1024)" fillcolor=lightblue]
	3115446553152 -> 3115521595328
	3115521595328 [label=AccumulateGrad]
	3115521595184 -> 3115521595040
	3115446553552 [label="layers.4.net.1.net.3.weight
 (1024, 1024, 3, 3)" fillcolor=lightblue]
	3115446553552 -> 3115521595184
	3115521595184 [label=AccumulateGrad]
	3115521595136 -> 3115521595040
	3115446553632 [label="layers.4.net.1.net.3.bias
 (1024)" fillcolor=lightblue]
	3115446553632 -> 3115521595136
	3115521595136 [label=AccumulateGrad]
	3115521594992 -> 3115521594944
	3115446553712 [label="layers.4.net.1.net.4.weight
 (1024)" fillcolor=lightblue]
	3115446553712 -> 3115521594992
	3115521594992 [label=AccumulateGrad]
	3115521594752 -> 3115521594944
	3115446553792 [label="layers.4.net.1.net.4.bias
 (1024)" fillcolor=lightblue]
	3115446553792 -> 3115521594752
	3115521594752 [label=AccumulateGrad]
	3115521594128 -> 3115521594032
	3115446554192 [label="layers.5.upsample.1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	3115446554192 -> 3115521594128
	3115521594128 [label=AccumulateGrad]
	3115521593648 -> 3115521594032
	3115446554272 [label="layers.5.upsample.1.bias
 (512)" fillcolor=lightblue]
	3115446554272 -> 3115521593648
	3115521593648 [label=AccumulateGrad]
	3115521585056 -> 3115521584912
	3115446554432 [label="layers.5.conv.net.0.weight
 (512, 1024, 3, 3)" fillcolor=lightblue]
	3115446554432 -> 3115521585056
	3115521585056 [label=AccumulateGrad]
	3115521585008 -> 3115521584912
	3115446554512 [label="layers.5.conv.net.0.bias
 (512)" fillcolor=lightblue]
	3115446554512 -> 3115521585008
	3115521585008 [label=AccumulateGrad]
	3115521584864 -> 3115521584816
	3115521372224 [label="layers.5.conv.net.1.weight
 (512)" fillcolor=lightblue]
	3115521372224 -> 3115521584864
	3115521584864 [label=AccumulateGrad]
	3115521584720 -> 3115521584816
	3115521372304 [label="layers.5.conv.net.1.bias
 (512)" fillcolor=lightblue]
	3115521372304 -> 3115521584720
	3115521584720 [label=AccumulateGrad]
	3115521584576 -> 3115521584432
	3115521372704 [label="layers.5.conv.net.3.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	3115521372704 -> 3115521584576
	3115521584576 [label=AccumulateGrad]
	3115521584528 -> 3115521584432
	3115521372784 [label="layers.5.conv.net.3.bias
 (512)" fillcolor=lightblue]
	3115521372784 -> 3115521584528
	3115521584528 [label=AccumulateGrad]
	3115521584384 -> 3115521584336
	3115521372864 [label="layers.5.conv.net.4.weight
 (512)" fillcolor=lightblue]
	3115521372864 -> 3115521584384
	3115521584384 [label=AccumulateGrad]
	3115521584144 -> 3115521584336
	3115521372944 [label="layers.5.conv.net.4.bias
 (512)" fillcolor=lightblue]
	3115521372944 -> 3115521584144
	3115521584144 [label=AccumulateGrad]
	3115521583664 -> 3115521583568
	3115521373424 [label="layers.6.upsample.1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	3115521373424 -> 3115521583664
	3115521583664 [label=AccumulateGrad]
	3115521583184 -> 3115521583568
	3115521373504 [label="layers.6.upsample.1.bias
 (256)" fillcolor=lightblue]
	3115521373504 -> 3115521583184
	3115521583184 [label=AccumulateGrad]
	3115521582848 -> 3115521582704
	3115521373664 [label="layers.6.conv.net.0.weight
 (256, 512, 3, 3)" fillcolor=lightblue]
	3115521373664 -> 3115521582848
	3115521582848 [label=AccumulateGrad]
	3115521582800 -> 3115521582704
	3115521373744 [label="layers.6.conv.net.0.bias
 (256)" fillcolor=lightblue]
	3115521373744 -> 3115521582800
	3115521582800 [label=AccumulateGrad]
	3115521582656 -> 3115521582608
	3115521373824 [label="layers.6.conv.net.1.weight
 (256)" fillcolor=lightblue]
	3115521373824 -> 3115521582656
	3115521582656 [label=AccumulateGrad]
	3115521582512 -> 3115521582608
	3115521373904 [label="layers.6.conv.net.1.bias
 (256)" fillcolor=lightblue]
	3115521373904 -> 3115521582512
	3115521582512 [label=AccumulateGrad]
	3115521582368 -> 3115521582224
	3115521374304 [label="layers.6.conv.net.3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	3115521374304 -> 3115521582368
	3115521582368 [label=AccumulateGrad]
	3115521582320 -> 3115521582224
	3115521374384 [label="layers.6.conv.net.3.bias
 (256)" fillcolor=lightblue]
	3115521374384 -> 3115521582320
	3115521582320 [label=AccumulateGrad]
	3115521582176 -> 3115521582128
	3115521374464 [label="layers.6.conv.net.4.weight
 (256)" fillcolor=lightblue]
	3115521374464 -> 3115521582176
	3115521582176 [label=AccumulateGrad]
	3115521582032 -> 3115521582128
	3115521374544 [label="layers.6.conv.net.4.bias
 (256)" fillcolor=lightblue]
	3115521374544 -> 3115521582032
	3115521582032 [label=AccumulateGrad]
	3115521581504 -> 3115521552288
	3115521374944 [label="layers.7.upsample.1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	3115521374944 -> 3115521581504
	3115521581504 [label=AccumulateGrad]
	3115521581216 -> 3115521552288
	3115521375024 [label="layers.7.upsample.1.bias
 (128)" fillcolor=lightblue]
	3115521375024 -> 3115521581216
	3115521581216 [label=AccumulateGrad]
	3115521551952 -> 3115521551808
	3115521375184 [label="layers.7.conv.net.0.weight
 (128, 256, 3, 3)" fillcolor=lightblue]
	3115521375184 -> 3115521551952
	3115521551952 [label=AccumulateGrad]
	3115521551904 -> 3115521551808
	3115521375264 [label="layers.7.conv.net.0.bias
 (128)" fillcolor=lightblue]
	3115521375264 -> 3115521551904
	3115521551904 [label=AccumulateGrad]
	3115521551760 -> 3115521551712
	3115521375344 [label="layers.7.conv.net.1.weight
 (128)" fillcolor=lightblue]
	3115521375344 -> 3115521551760
	3115521551760 [label=AccumulateGrad]
	3115521551616 -> 3115521551712
	3115521375424 [label="layers.7.conv.net.1.bias
 (128)" fillcolor=lightblue]
	3115521375424 -> 3115521551616
	3115521551616 [label=AccumulateGrad]
	3115521551472 -> 3115521551328
	3115521375824 [label="layers.7.conv.net.3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	3115521375824 -> 3115521551472
	3115521551472 [label=AccumulateGrad]
	3115521551424 -> 3115521551328
	3115521375904 [label="layers.7.conv.net.3.bias
 (128)" fillcolor=lightblue]
	3115521375904 -> 3115521551424
	3115521551424 [label=AccumulateGrad]
	3115521551280 -> 3115521551232
	3115521375984 [label="layers.7.conv.net.4.weight
 (128)" fillcolor=lightblue]
	3115521375984 -> 3115521551280
	3115521551280 [label=AccumulateGrad]
	3115521551040 -> 3115521551232
	3115521376064 [label="layers.7.conv.net.4.bias
 (128)" fillcolor=lightblue]
	3115521376064 -> 3115521551040
	3115521551040 [label=AccumulateGrad]
	3115521550656 -> 3115521550560
	3115521466672 [label="layers.8.upsample.1.weight
 (64, 128, 1, 1)" fillcolor=lightblue]
	3115521466672 -> 3115521550656
	3115521550656 [label=AccumulateGrad]
	3115521550176 -> 3115521550560
	3115521466752 [label="layers.8.upsample.1.bias
 (64)" fillcolor=lightblue]
	3115521466752 -> 3115521550176
	3115521550176 [label=AccumulateGrad]
	3115521549840 -> 3115521549696
	3115521466912 [label="layers.8.conv.net.0.weight
 (64, 128, 3, 3)" fillcolor=lightblue]
	3115521466912 -> 3115521549840
	3115521549840 [label=AccumulateGrad]
	3115521549792 -> 3115521549696
	3115521466992 [label="layers.8.conv.net.0.bias
 (64)" fillcolor=lightblue]
	3115521466992 -> 3115521549792
	3115521549792 [label=AccumulateGrad]
	3115521549648 -> 3115521549600
	3115521467072 [label="layers.8.conv.net.1.weight
 (64)" fillcolor=lightblue]
	3115521467072 -> 3115521549648
	3115521549648 [label=AccumulateGrad]
	3115521549504 -> 3115521549600
	3115521467152 [label="layers.8.conv.net.1.bias
 (64)" fillcolor=lightblue]
	3115521467152 -> 3115521549504
	3115521549504 [label=AccumulateGrad]
	3115521549360 -> 3115521549216
	3115521467552 [label="layers.8.conv.net.3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	3115521467552 -> 3115521549360
	3115521549360 [label=AccumulateGrad]
	3115521549312 -> 3115521549216
	3115521467632 [label="layers.8.conv.net.3.bias
 (64)" fillcolor=lightblue]
	3115521467632 -> 3115521549312
	3115521549312 [label=AccumulateGrad]
	3115521549168 -> 3115521549120
	3115521467712 [label="layers.8.conv.net.4.weight
 (64)" fillcolor=lightblue]
	3115521467712 -> 3115521549168
	3115521549168 [label=AccumulateGrad]
	3115521548880 -> 3115521549120
	3115521467792 [label="layers.8.conv.net.4.bias
 (64)" fillcolor=lightblue]
	3115521467792 -> 3115521548880
	3115521548880 [label=AccumulateGrad]
	3115521549024 -> 3115521548976
	3115521468192 [label="layers.9.weight
 (3, 64, 1, 1)" fillcolor=lightblue]
	3115521468192 -> 3115521549024
	3115521549024 [label=AccumulateGrad]
	3115521548928 -> 3115521548976
	3115521468272 [label="layers.9.bias
 (3)" fillcolor=lightblue]
	3115521468272 -> 3115521548928
	3115521548928 [label=AccumulateGrad]
	3115521548976 -> 3115521373264
}
